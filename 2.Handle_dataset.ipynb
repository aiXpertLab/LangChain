{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF_DATASETS_CACHE='/mnt/e/models/huggingface.cache'\n",
    "# HF_HOME='/mnt/e/models/huggingface.cache'\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "from langchain_community.llms       import LlamaCpp, CTransformers\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_community.embeddings import LlamaCppEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader, WebBaseLoader, PyPDFDirectoryLoader, CSVLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.prompts       import PromptTemplate\n",
    "from langchain.chains        import LLMChain, RetrievalQA\n",
    "#1m 1.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2431ff4a75d429795ef11b70aeacf4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbae8f627f04248ab4596f52d839ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"HuggingFaceTB/cosmopedia\", \"stories\", split=\"train[:100]\")\n",
    "num_examples = len(data)\n",
    "\n",
    "# data = data.to_pandas()\n",
    "# data.to_csv(\"E:/models/huggingface.cache/HuggingFaceTB___cosmopedia/stories/dataset.csv\")\n",
    "print(num_examples)\n",
    "#3.4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path='/mnt/e/models/huggingface.cache/HuggingFaceTB___cosmopedia/stories/dataset5100.csv')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "# model_kwargs = {'device':'cpu'}\n",
    "model_kwargs = {'device': 'cuda:0'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    " model_name=modelPath, \n",
    " model_kwargs=model_kwargs, \n",
    " encode_kwargs=encode_kwargs \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, embeddings)\n",
    "# db.save_local(\"E:/models/huggingface.cache/HuggingFaceTB___cosmopedia/stories/faiss_index_constitution\")\n",
    "# vectorstore.save_local(\"faiss_index_constitution\")\n",
    "\n",
    "#     # Load from local storage\n",
    "#     persisted_vectorstore = FAISS.load_local(\"faiss_index_constitution\", embeddings)\n",
    "\n",
    "#     # Use RetrievalQA chain for orchestration\n",
    "#     qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=persisted_vectorstore.as_retriever())\n",
    "#     result = qa.run(query)\n",
    "#     print(result)\n",
    "# 1m 12.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fb16ef3f3145a7ab25a4991b539fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/mnt/e/models/2b-gemma\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "#2m 35.3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_tensors='pt',\n",
    "    max_length=512,\n",
    "    max_new_tokens=512,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    # device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(\n",
    " pipeline=pipe,\n",
    " model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Write a story.',\n",
       " 'result': \"\\n\\nSure, here is a story written based on the text snippet:\\n\\nIn a quaint subreddit where stories flourished, I stumbled upon a thread that captured my attention. It was a tale of a young writer who faced adversity and emerged as a champion. The story resonated deeply with me, as it reminded me of the importance of perseverance and the power of storytelling.\\n\\nThe writer's journey was marked by countless obstacles, including financial struggles and rejection from publishers. Yet, through sheer determination and unwavering belief in his craft, he overcame these challenges. His story served as a reminder that even in the face of adversity, the human spirit can triumph.\\n\\nThe writer's triumph inspired a community of readers, who shared their own stories of resilience and triumph. Together, we formed a bond of shared experiences, realizing that stories serve as a source of solace, inspiration, and a reminder that we are not alone in our journeys.\\n\\nThe thread became a beacon of hope, reminding me that stories have the power to unite, inspire, and provide solace to those who seek them. They are a reminder that we are all connected by the threads of shared experiences, and that our stories, no matter how seemingly insignificant, have the potential to make a profound difference.\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.invoke(\"Write a story.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
