{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ai.plainenglish.io/%EF%B8%8F-langchain-streamlit-llama-bringing-conversational-ai-to-your-local-machine-a1736252b172\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# model_name = \"llama-2-7b-chat.Q8_0.gguf\"\n",
    "model_name = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "\n",
    "llm = Llama(model_path=f\"e:/models/llama/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1087.47 ms\n",
      "llama_print_timings:      sample time =       3.16 ms /    16 runs   (    0.20 ms per token,  5058.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1087.42 ms /     7 tokens (  155.35 ms per token,     6.44 tokens per second)\n",
      "llama_print_timings:        eval time =    2467.71 ms /    15 runs   (  164.51 ms per token,     6.08 tokens per second)\n",
      "llama_print_timings:       total time =    3597.99 ms /    22 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Pizza (32%)\n",
      "2. Burgers (2\n"
     ]
    }
   ],
   "source": [
    "response = llm(\"Canadian most favorite food is:\")\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about chickens.\n"
     ]
    }
   ],
   "source": [
    "# PromptTemplate\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "template = \"Tell me a {adj} joke about {content}.\"\n",
    "prompt=PromptTemplate.from_template(template)\n",
    "print(prompt.format(adj = 'funny', content ='chickens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from e:/models/llama/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     1.10 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "llm = LlamaCpp(model_path=f\"e:/models/llama/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.llms.llamacpp.LlamaCpp'>\n"
     ]
    }
   ],
   "source": [
    "print(type(llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Name one of the Most beautiful women in China\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Q: Name one of the Most beautiful women in {country}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "formatted_prompt = prompt.format(country = 'China')\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1028.62 ms\n",
      "llama_print_timings:      sample time =      17.95 ms /    88 runs   (    0.20 ms per token,  4901.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1645.22 ms /    15 tokens (  109.68 ms per token,     9.12 tokens per second)\n",
      "llama_print_timings:        eval time =   13827.07 ms /    87 runs   (  158.93 ms per token,     6.29 tokens per second)\n",
      "llama_print_timings:       total time =   15716.98 ms /   102 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Liu Yifei, also known as Crystal Liu, is a Chinese actress and model who has been named as one of the most beautiful women in China. She was born on January 25, 1987, in Wuhan, China, and has won numerous awards for her acting talent and beauty, including Best Actress at the Hong Kong Film Awards and Top China Model at the China Model Awards.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt=formatted_prompt,  stop = [\"Q:\", \"\\n\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1028.62 ms\n",
      "llama_print_timings:      sample time =      53.94 ms /   256 runs   (    0.21 ms per token,  4745.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     366.49 ms /     4 tokens (   91.62 ms per token,    10.91 tokens per second)\n",
      "llama_print_timings:        eval time =   41251.59 ms /   255 runs   (  161.77 ms per token,     6.18 tokens per second)\n",
      "llama_print_timings:       total time =   42374.50 ms /   259 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' There are many beautiful women in Canada, and it\\'s difficult to pinpoint just one as the most beautiful. However, some notable Canadian celebrities known for their beauty include:\\n1. Rachel McAdams - Actress and model known for her roles in \"Mean Girls,\" \"The Notebook,\" and \"Midnight in Paris.\"\\n2. Pamela Anderson - Model and actress known for her roles on \"Baywatch\" and \"Home Improvement.\"\\n3. Celine Dion - Singer and songwriter known for her powerful voice and iconic hits such as \"My Heart Will Go On\" and \"Because You Loved Me.\"\\n4. Sarah Gadon - Actress known for her roles in \"A Dangerous Method\" and \"The Amazing Spider-Man.\"\\n5. Kristen Johnston - Actress known for her roles on \"The Exes\" and \"Murder in the First.\"\\n6. Sandra Oh - Actress known for her roles on \"Grey\\'s Anatomy\" and \"Killing Eve.\"\\n7. Carly Rae Jepsen - Singer-songwriter known for her hit single \"Call Me Maybe.\"'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt,llm=llm)\n",
    "llm_chain.run(\"Canada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 104, which is longer than the specified 10\n",
      "Created a chunk of size 295, which is longer than the specified 10\n",
      "Created a chunk of size 228, which is longer than the specified 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Batman does not possess any superpowers, instead relying on his intellect, fighting skills, and wealth. \\n\\nAs a baby, his parents sent him to Earth in a small spaceship moments before Krypton was destroyed in a natural cataclysm. His ship landed in the American countryside, near the fictional town of Smallville. He was found and adopted by farmers Jonathan and Martha Kent, who named him Clark Kent. \\n\\nIn her homeland, the island nation of Themyscira, her official title is Princess Diana of Themyscira. She is known for her compassion, strength, and commitment to justice, serving as a symbol of female empowerment and equality. \\n\\nThis new Flash was Barry Allen, a police scientist who gained super-speed when bathed by chemicals after a shelf of them was struck by lightning. He adopted the name The Scarlet Speedster after reading a comic book featuring the Golden Age Flash. \\n\\n', metadata={'source': './docs/raw.txt'})]\n",
      "1\n",
      "[Document(page_content='Batman does not possess any superpowers, instead relying on his intellect, fighting skills, and wealth.', metadata={'source': './docs/raw.txt'}), Document(page_content='As a baby, his parents sent him to Earth in a small spaceship moments before Krypton was destroyed in a natural cataclysm. His ship landed in the American countryside, near the fictional town of Smallville. He was found and adopted by farmers Jonathan and Martha Kent, who named him Clark Kent.', metadata={'source': './docs/raw.txt'}), Document(page_content='In her homeland, the island nation of Themyscira, her official title is Princess Diana of Themyscira. She is known for her compassion, strength, and commitment to justice, serving as a symbol of female empowerment and equality.', metadata={'source': './docs/raw.txt'}), Document(page_content='This new Flash was Barry Allen, a police scientist who gained super-speed when bathed by chemicals after a shelf of them was struck by lightning. He adopted the name The Scarlet Speedster after reading a comic book featuring the Golden Age Flash.', metadata={'source': './docs/raw.txt'})]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# text\n",
    "\n",
    "# pip install langchain==0.1.6\n",
    "# pip install langchain-community==0.0.19\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('./docs/raw.txt')\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size = 10, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "\n",
    "print(docs)\n",
    "print(len(docs))\n",
    "\n",
    "print(texts)\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from e:/models/llama/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     1.10 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Batman does not possess any superpowers, instead relying on his intellect, fighting skills, and wealth.' metadata={'source': './docs/raw.txt'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "embeddings = LlamaCppEmbeddings(model_path=f\"e:/models/llama/{model_name}\")\n",
    "\n",
    "_texts = []\n",
    "for i in range(len(texts)):\n",
    "    _texts.append(texts[i].page_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Batman does not possess any superpowers, instead relying on his intellect, fighting skills, and wealth.' metadata={'source': './docs/raw.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     938.54 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     937.40 ms /     8 tokens (  117.17 ms per token,     8.53 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     955.51 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =     938.54 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     662.35 ms /     8 tokens (   82.79 ms per token,    12.08 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     664.85 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =     938.54 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     644.82 ms /     8 tokens (   80.60 ms per token,    12.41 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     645.65 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =     938.54 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     656.83 ms /     8 tokens (   82.10 ms per token,    12.18 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     656.99 ms /     9 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4096\n"
     ]
    }
   ],
   "source": [
    "embedded_texts = embeddings.embed_documents(_texts)\n",
    "print(len(embedded_texts))\n",
    "print(len(embedded_texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     938.54 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     629.86 ms /     8 tokens (   78.73 ms per token,    12.70 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     629.89 ms /     9 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "source": [
    "query = 'what skills did batman had?'\n",
    "embedded_query = embeddings.embed_query(query)\n",
    "print(len(embedded_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python312\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_validation.py:26: UserWarning: Unsupported Windows version (11). ONNX Runtime supports Windows 10 and above, only.\n",
      "  warnings.warn(\n",
      "\n",
      "llama_print_timings:        load time =     938.54 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     636.56 ms /     8 tokens (   79.57 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     636.42 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =     938.54 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     665.79 ms /     8 tokens (   83.22 ms per token,    12.02 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     666.69 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =     938.54 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     620.11 ms /     8 tokens (   77.51 ms per token,    12.90 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     620.31 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =     938.54 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     639.17 ms /     8 tokens (   79.90 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     639.32 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =     938.54 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     566.68 ms /     7 tokens (   80.95 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     566.39 ms /     8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='In her homeland, the island nation of Themyscira, her official title is Princess Diana of Themyscira. She is known for her compassion, strength, and commitment to justice, serving as a symbol of female empowerment and equality.', metadata={'source': './docs/raw.txt'})]\n"
     ]
    }
   ],
   "source": [
    "#Creating Vector Store & Retrieving Docs\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "query = \"who is an orphan here\"\n",
    "docs = db.similarity_search(query, k=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='In her homeland, the island nation of Themyscira, her official title is Princess Diana of Themyscira. She is known for her compassion, strength, and commitment to justice, serving as a symbol of female empowerment and equality.', metadata={'source': './docs/raw.txt'})]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'PromptTemplate' has no attribute 'from_tempalte'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# from langchain.chains import LLMChain\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# from langchain.prompts import PromptTemplate\u001b[39;00m\n\u001b[0;32m      4\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mUse the following pieces of context to answer the qeustion at the end.\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mIf you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know the answer, just say that you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know, don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt try to make up an answer.\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;132;01m{context}\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124mAnser:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m prompt\u001b[38;5;241m=\u001b[39m\u001b[43mPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tempalte\u001b[49m(template)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt\u001b[38;5;241m.\u001b[39minput_variables)\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'PromptTemplate' has no attribute 'from_tempalte'"
     ]
    }
   ],
   "source": [
    "# from langchain.chains import LLMChain\n",
    "# from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the qeustion at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Anser:\"\"\"\n",
    "\n",
    "prompt=PromptTemplate.from_tempalte(template)\n",
    "print(prompt.input_variables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
